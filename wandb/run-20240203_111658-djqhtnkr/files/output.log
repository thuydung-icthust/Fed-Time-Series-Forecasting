INFO logger 2024-02-03 11:17:02,675 | data_utils.py:383 | Observations info in ElBorn
INFO logger 2024-02-03 11:17:02,675 | data_utils.py:384 | 	Total number of samples:  4192
INFO logger 2024-02-03 11:17:02,675 | data_utils.py:385 | 	Number of samples for training: 3354
INFO logger 2024-02-03 11:17:02,675 | data_utils.py:386 | 	Number of samples for validation:  838
INFO logger 2024-02-03 11:17:02,677 | data_utils.py:383 | Observations info in LesCorts
INFO logger 2024-02-03 11:17:02,677 | data_utils.py:384 | 	Total number of samples:  6892
INFO logger 2024-02-03 11:17:02,677 | data_utils.py:385 | 	Number of samples for training: 5514
INFO logger 2024-02-03 11:17:02,677 | data_utils.py:386 | 	Number of samples for validation:  1378
INFO logger 2024-02-03 11:17:02,679 | data_utils.py:383 | Observations info in PobleSec
INFO logger 2024-02-03 11:17:02,679 | data_utils.py:384 | 	Total number of samples:  15927
INFO logger 2024-02-03 11:17:02,679 | data_utils.py:385 | 	Number of samples for training: 12742
INFO logger 2024-02-03 11:17:02,679 | data_utils.py:386 | 	Number of samples for validation:  3185
INFO logger 2024-02-03 11:17:02,680 | data_utils.py:389 | Observations info using all data
INFO logger 2024-02-03 11:17:02,681 | data_utils.py:390 | 	Total number of samples:  27011
INFO logger 2024-02-03 11:17:02,681 | data_utils.py:391 | 	Number of samples for training: 21610
INFO logger 2024-02-03 11:17:02,681 | data_utils.py:392 | 	Number of samples for validation:  5401
INFO logger 2024-02-03 11:17:02,681 | data_utils.py:118 | Using Flooring and Capping and with params: {'ElBorn': (10, 90), 'LesCorts': (10, 90), 'PobleSec': (5, 95)}
INFO logger 2024-02-03 11:17:03,367 | server.py:74 | Initializing client manager...
INFO logger 2024-02-03 11:17:03,367 | server.py:81 | Registering clients...
INFO logger 2024-02-03 11:17:03,368 | client_manager.py:66 | Registered client with id: ElBorn_0
INFO logger 2024-02-03 11:17:03,368 | client_manager.py:66 | Registered client with id: ElBorn_1
INFO logger 2024-02-03 11:17:03,368 | client_manager.py:66 | Registered client with id: ElBorn_2
INFO logger 2024-02-03 11:17:03,368 | client_manager.py:66 | Registered client with id: ElBorn_3
INFO logger 2024-02-03 11:17:03,368 | client_manager.py:66 | Registered client with id: ElBorn_4
INFO logger 2024-02-03 11:17:03,369 | client_manager.py:66 | Registered client with id: LesCorts_0
INFO logger 2024-02-03 11:17:03,369 | client_manager.py:66 | Registered client with id: LesCorts_1
INFO logger 2024-02-03 11:17:03,369 | client_manager.py:66 | Registered client with id: LesCorts_2
INFO logger 2024-02-03 11:17:03,369 | client_manager.py:66 | Registered client with id: LesCorts_3
INFO logger 2024-02-03 11:17:03,369 | client_manager.py:66 | Registered client with id: LesCorts_4
INFO logger 2024-02-03 11:17:03,369 | client_manager.py:66 | Registered client with id: PobleSec_0
INFO logger 2024-02-03 11:17:03,370 | client_manager.py:66 | Registered client with id: PobleSec_1
INFO logger 2024-02-03 11:17:03,370 | client_manager.py:66 | Registered client with id: PobleSec_2
INFO logger 2024-02-03 11:17:03,370 | client_manager.py:66 | Registered client with id: PobleSec_3
INFO logger 2024-02-03 11:17:03,370 | client_manager.py:66 | Registered client with id: PobleSec_4
INFO logger 2024-02-03 11:17:03,370 | server.py:85 | Client manager initialized!
INFO logger 2024-02-03 11:17:03,370 | server.py:67 | Aggregation algorithm: FedAvg()
INFO logger 2024-02-03 11:17:03,371 | client_manager.py:88 | Parameter c=0.0. Sampled 1 client(s): ['PobleSec_3']
X_val_sub.shape is 535 	 y_val_sub is 535
Clients participating training are: dict_keys(['ElBorn', 'LesCorts', 'PobleSec'])
INFO logger 2024-02-03 11:17:04,573 | server.py:99 | Starting FL rounds
INFO logger 2024-02-03 11:17:04,573 | client_manager.py:88 | Parameter c=1.0. Sampled 15 client(s): ['LesCorts_1', 'PobleSec_2', 'PobleSec_4', 'ElBorn_0', 'ElBorn_4', 'LesCorts_3', 'LesCorts_2', 'PobleSec_3', 'LesCorts_4', 'ElBorn_2', 'ElBorn_3', 'LesCorts_0', 'PobleSec_0', 'PobleSec_1', 'ElBorn_1']
INFO logger 2024-02-03 11:17:05,325 | train_utils.py:153 | Best Loss: 0.0002896100798708915
INFO logger 2024-02-03 11:17:05,917 | train_utils.py:153 | Best Loss: 0.00048219764949827213
INFO logger 2024-02-03 11:17:06,679 | train_utils.py:153 | Best Loss: 0.0004605070486166111
INFO logger 2024-02-03 11:17:07,200 | train_utils.py:153 | Best Loss: 0.0002248865115829712
INFO logger 2024-02-03 11:17:07,463 | train_utils.py:153 | Best Loss: 2.704441241170887e-05
INFO logger 2024-02-03 11:17:08,083 | train_utils.py:153 | Best Loss: 0.0005417556082557516
[31mStarting poisoning...!
[31mStarting poisoning...!
INFO logger 2024-02-03 11:17:08,385 | train_utils.py:153 | Best Loss: 0.00021486838037769
INFO logger 2024-02-03 11:17:09,117 | train_utils.py:153 | Best Loss: 0.00046282758760288006
INFO logger 2024-02-03 11:17:09,604 | train_utils.py:153 | Best Loss: 0.00017418319152461158
INFO logger 2024-02-03 11:17:09,844 | train_utils.py:153 | Best Loss: 2.86163878345925e-05
INFO logger 2024-02-03 11:17:10,116 | train_utils.py:153 | Best Loss: 0.0001446908842637271
[31mStarting poisoning...!
[31mStarting poisoning...!
INFO logger 2024-02-03 11:17:10,522 | train_utils.py:153 | Best Loss: 0.0007182669863976234
INFO logger 2024-02-03 11:17:11,085 | train_utils.py:153 | Best Loss: 0.000442783231663657
INFO logger 2024-02-03 11:17:11,971 | train_utils.py:153 | Best Loss: 0.0004338021676136753
INFO logger 2024-02-03 11:17:12,543 | train_utils.py:153 | Best Loss: 2.6967046345957523e-05
preds shape is: (15, 535, 5)
preds shape by feat is: (15, 535)
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
end_idx: 20
end_idx: 40
end_idx: 60
end_idx: 80
end_idx: 100
end_idx: 120
end_idx: 140
end_idx: 160
end_idx: 180
end_idx: 200
end_idx: 220
end_idx: 240
end_idx: 260
end_idx: 280
end_idx: 300
end_idx: 320
end_idx: 340
end_idx: 360
end_idx: 380
end_idx: 400
end_idx: 420
end_idx: 440
end_idx: 460
end_idx: 480
end_idx: 500
end_idx: 520
end_idx: 535
Python 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.17.2 -- An enhanced Interactive Python. Type '?' for help.
Python 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.17.2 -- An enhanced Interactive Python. Type '?' for help.
(535, 5)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~/code/Fed-Time-Series-Forecasting/main.py:131
    128     args.outlier_columns = outlier_columns
    129     args.outlier_kwargs = outlier_kwargs
--> 131 main(args)
File ~/code/Fed-Time-Series-Forecasting/main.py:54, in main(args)
     49 # federated local params
     50 local_train_params = {"epochs": args.epochs, "optimizer": args.optimizer, "lr": args.lr,
     51                     "criterion": args.criterion, "early_stopping": args.local_early_stopping,
     52                     "patience": args.local_patience, "device": args.device
     53                     }
---> 54 global_model, history = fit(
     55                             model,
     56                             client_X_train,
     57                             client_y_train,
     58                             client_X_val,
     59                             client_y_val,
     60                             local_train_params=local_train_params,
     61                             args = args,
     62                             wandb_ins=wandb_instance,
     63                             subval_dataloader=subval_dataloader)
File ~/code/Fed-Time-Series-Forecasting/fl_trainer.py:137, in fit(model, X_train, y_train, X_val, y_val, exogenous_data_train, exogenous_data_val, idxs, log_per, client_creation_fn, local_train_params, aggregation_params, use_carbontracker, args, wandb_ins, subval_dataloader)
    125 server = Server(
    126     client_proxies=client_proxies, # the client representations
    127     aggregation=args.aggregation, # the aggregation algorithm
   (...)
    132     subval_dataloader=subval_dataloader
    133 )
    134 # Note that the client manager instance will be initialized automatically. You can define your own client manager.
    135
    136 # train with FL
--> 137 model_params, history = server.fit(args.fl_rounds, args.fraction, use_carbontracker=use_carbontracker,
    138                                    wandb_ins=wandb_ins)
    140 params_dict = zip(model.state_dict().keys(), model_params)
    141 state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
File ~/code/Fed-Time-Series-Forecasting/ml/fl/server/server.py:114, in Server.fit(self, num_rounds, fraction, fraction_args, use_carbontracker, wandb_ins)
    112     cb_tracker.epoch_start()
    113 # train and replace the previous global model
--> 114 self.fit_round(fl_round=fl_round,
    115                fraction=fraction,
    116                fraction_args=fraction_args,
    117                history=history)
    118 if use_carbontracker and cb_tracker is not None:
    119     cb_tracker.epoch_end()
File ~/code/Fed-Time-Series-Forecasting/ml/fl/server/server.py:186, in Server.fit_round(self, fl_round, fraction, fraction_args, history)
    183 history.add_local_test_metrics(all_test_metrics, fl_round)
    185 # Conduct defense before aggregation
--> 186 self.conduct_defense(model_vec_list)
    187 # STEP 4: Aggregate local models
    188 self.global_model = self.aggregate_models(fl_round, results)
File ~/code/Fed-Time-Series-Forecasting/ml/fl/server/server.py:224, in Server.conduct_defense(self, local_weights)
    222     y_true = y_true.cpu().numpy()
    223     preds.append(y_pred)
--> 224 verification_result = sat_verifier(preds, y_true, 0)
File ~/code/Fed-Time-Series-Forecasting/verifier.py:116, in sat_verifier(pred_list, y_true, atk_feature)
    114 preds_by_feat = preds[:, :, atk_feature]
    115 print(f"preds shape by feat is: {preds_by_feat.shape}")
--> 116 r_a = verify(preds_by_feat)
    117 print(r_a)
File ~/code/Fed-Time-Series-Forecasting/verifier.py:51, in verify(preds)
     48 aggregated_lb, aggregated_ub = rule_aggregation(LBs, UBs)
     49 # import IPython
     50 # IPython.embed()
---> 51 verifier_results = get_satisfied_elements(preds, np.asarray(aggregated_lb), np.asarray(aggregated_ub))
     52 return verifier_results
File ~/code/Fed-Time-Series-Forecasting/verifier.py:65, in get_satisfied_elements(predictions, lb, ub)
     63     import IPython
     64     IPython.embed()
---> 65     satisfied_elements = [1.0 if lb[i] <= pred[i] <= ub[i] else 0.0 for i in range(len(pred))]
     66     verifier_results.append(satisfied_elements)
     67 return verifier_results
File ~/code/Fed-Time-Series-Forecasting/verifier.py:65, in <listcomp>(.0)
     63     import IPython
     64     IPython.embed()
---> 65     satisfied_elements = [1.0 if lb[i] <= pred[i] <= ub[i] else 0.0 for i in range(len(pred))]
     66     verifier_results.append(satisfied_elements)
     67 return verifier_results
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()